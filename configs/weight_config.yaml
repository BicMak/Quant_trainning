# Weight Quantization Configuration
# Linear layer and LayerNorm weight quantization settings

# ==================== Global Default Configuration ====================
default:
  calibration_mode: channel_wise
  bit_type:
    bits: 8
    signed: true
    name: int8
  observer_type: PercentileObserver
  quantization_method: Uniform
  percentile_alpha: 0.999
  percentile_sigma: 0.01
  kl_bins: 2048

# ==================== Weight Layers ====================
layers:
  # Attention QKV projection (Query, Key, Value)
  attn_qkv:
    calibration_mode: channel_wise
    bit_type:
      bits: 8
      signed: true
      name: int8
    observer_type: PercentileObserver
    quantization_method: Uniform
    percentile_alpha: 0.999
    percentile_sigma: 0.2

  # Attention output projection
  attn_proj:
    calibration_mode: channel_wise
    bit_type:
      bits: 8
      signed: true
      name: int8
    observer_type: PercentileObserver
    quantization_method: Uniform
    percentile_alpha: 0.995
    percentile_sigma: 0.2

  # MLP first linear layer (expansion)
  mlp_fc1:
    calibration_mode: channel_wise
    bit_type:
      bits: 8
      signed: true
      name: int8
    observer_type: PercentileObserver
    quantization_method: Uniform
    percentile_alpha: 0.999
    percentile_sigma: 0.2

  # MLP second linear layer (projection)
  mlp_fc2:
    calibration_mode: channel_wise
    bit_type:
      bits: 8
      signed: true
      name: int8
    observer_type: PercentileObserver
    quantization_method: Uniform
    percentile_alpha: 0.999
    percentile_sigma: 0.2

  # LayerNorm 1 (before attention)
  norm1:
    calibration_mode: channel_wise
    bit_type:
      bits: 8
      signed: true
      name: int8
    observer_type: PercentileObserver
    quantization_method: Uniform
    percentile_alpha: 0.999
    percentile_sigma: 0.2

  # LayerNorm 2 (before MLP)
  norm2:
    calibration_mode: channel_wise
    bit_type:
      bits: 8
      signed: true
      name: int8
    observer_type: PercentileObserver
    quantization_method: Uniform
    percentile_alpha: 0.999
    percentile_sigma: 0.2
