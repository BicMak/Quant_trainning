# Activation Quantization Configuration
# Activation quantization settings for intermediate outputs

# ==================== Global Default Configuration ====================
default:
  calibration_mode: channel_wise
  bit_type:
    bits: 8
    signed: true
    name: int8
  observer_type: MinmaxObserver
  quantization_method: Uniform
  percentile_alpha: 0.9995
  percentile_sigma: 0.1
  kl_bins: 2048

# ==================== Activation Layers ====================
attn_layer:
  # QKV output after linear projection
  attn_qkv_output:
    calibration_mode: channel_wise
    bit_type:
      bits: 16
      signed: true
      name: int16
    observer_type: MinmaxObserver
    quantization_method: Uniform
    percentile_alpha: 0.999
    percentile_sigma: 0.2

  # Key-Value activation (after split)
  kv_act:
    calibration_mode: channel_wise
    bit_type:
      bits: 16
      signed: true
      name: int16
    observer_type: MinmaxObserver
    quantization_method: Uniform
    percentile_alpha: 0.999
    percentile_sigma: 0.2

  # Softmax-Value attention output
  sv_attn:
    calibration_mode: channel_wise
    bit_type:
      bits: 16
      signed: true
      name: int16
    observer_type: MinmaxObserver
    quantization_method: Uniform
    percentile_alpha: 0.999
    percentile_sigma: 0.2

  # MLP first activation (after GELU)
  mlp_act:
    calibration_mode: channel_wise
    bit_type:
      bits: 16
      signed: true
      name: int16
    observer_type: MinmaxObserver
    quantization_method: Uniform
    percentile_alpha: 0.999
    percentile_sigma: 0.2

  # MLP second activation (if exists)
  mlp_act2:
    calibration_mode: channel_wise
    bit_type:
      bits: 16
      signed: true
      name: int16
    observer_type: MinmaxObserver
    quantization_method: Uniform
    percentile_alpha: 0.999
    percentile_sigma: 0.2

  # IntSoftmax quantization
  intSoft:
    calibration_mode: layer_wise
    bit_type:
      bits: 4
      signed: true
      name: int4
    observer_type: MinmaxObserver
    quantization_method: Uniform
    percentile_alpha: 0.999
    percentile_sigma: 0.2
