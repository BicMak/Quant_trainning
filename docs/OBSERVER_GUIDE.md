# Observer ì‚¬ìš© ê°€ì´ë“œ

## ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [Observer íƒ€ì… ë¹„êµ](#observer-íƒ€ì…-ë¹„êµ)
3. [YAML ì„¤ì • ë°©ë²•](#yaml-ì„¤ì •-ë°©ë²•)
4. [ì½”ë“œ ì‚¬ìš© ì˜ˆì‹œ](#ì½”ë“œ-ì‚¬ìš©-ì˜ˆì‹œ)
5. [íŒŒë¼ë¯¸í„° ìƒì„¸ ì„¤ëª…](#íŒŒë¼ë¯¸í„°-ìƒì„¸-ì„¤ëª…)
6. [ì‹¤ì „ ê°€ì´ë“œ](#ì‹¤ì „-ê°€ì´ë“œ)

---

## ê°œìš”

**Observer**ëŠ” quantization ê³¼ì •ì—ì„œ activation/weightì˜ í†µê³„ë¥¼ ìˆ˜ì§‘í•˜ê³ , ìµœì ì˜ scaleê³¼ zero_pointë¥¼ ê³„ì‚°í•˜ëŠ” í•µì‹¬ ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤.

### Calibration ì›Œí¬í”Œë¡œìš°

```python
# Phase 1: Calibration - í†µê³„ ìˆ˜ì§‘
model.calibration(calibration_data)  # Observer.update() í˜¸ì¶œ

# Phase 2: Parameter Computation - scale/zero_point ê³„ì‚°
model.compute_quant_params()         # Observer.get_quantization_params() í˜¸ì¶œ

# Phase 3: Inference - ì‹¤ì œ quantization
model.mode = 'quantized'
output = model(input)
```

---

## Observer íƒ€ì… ë¹„êµ

| Observer | ì†ë„ | ì •í™•ë„ | Weight | Activation | ì£¼ìš” íŠ¹ì§• |
|----------|------|--------|--------|------------|-----------|
| **MinmaxObserver** | âš¡âš¡âš¡ | â­â­ | âœ… | âœ… | ë‹¨ìˆœ min/max, outlierì— ë¯¼ê° |
| **PercentileObserver** | âš¡âš¡ | â­â­â­ | âœ… | âœ… | Percentile clipping, outlier ê°•ê±´, **ê¸°ë³¸ê°’** |
| **OmseObserver** | âš¡ | â­â­â­â­ | âœ… | âœ… | Grid search + MSE ìµœì†Œí™”, ëŠë¦¬ì§€ë§Œ ì •í™• |
| **KLObserver** | âš¡ | â­â­â­â­ | âœ… | âœ… | KL divergence ìµœì†Œí™”, ë¶„í¬ ìœ ì§€ |

### ì‚¬ìš© ì‹œê¸° ê°€ì´ë“œ

```yaml
# ğŸ”¥ ì¶”ì²œ ì„¤ì • (ëŒ€ë¶€ë¶„ì˜ ê²½ìš°)
default:
  observer_type: PercentileObserver
  percentile_alpha: 0.9995  # 99.95% ë²”ìœ„ ì‚¬ìš©

# ğŸ¯ ì •í™•ë„ê°€ ì¤‘ìš”í•œ ê²½ìš° (weight quantization)
layers:
  attn_qkv:
    observer_type: OmseObserver  # ëŠë¦¬ì§€ë§Œ ê°€ì¥ ì •í™•

# ğŸ“Š ë¶„í¬ ìœ ì§€ê°€ ì¤‘ìš”í•œ ê²½ìš° (softmax, LayerNorm)
attn_layer:
  intSoft:
    observer_type: KLObserver
    kl_bins: 2048  # íˆìŠ¤í† ê·¸ë¨ bin ìˆ˜
```

---

## YAML ì„¤ì • ë°©ë²•

### 1. MinmaxObserver - ê¸°ë³¸ ì„¤ì •

```yaml
# configs/quant_config_minmax.yaml
default:
  calibration_mode: channel_wise  # or layer_wise
  bit_type:
    bits: 8
    signed: true
    name: int8
  observer_type: MinmaxObserver
  quantization_method: Uniform

layers:
  attn_qkv:
    # MinmaxObserverëŠ” ë³„ë„ íŒŒë¼ë¯¸í„° ì—†ìŒ
    observer_type: MinmaxObserver
```

**ì¥ì **: ê°€ì¥ ë¹ ë¦„, êµ¬í˜„ ê°„ë‹¨
**ë‹¨ì **: Outlierì— ë§¤ìš° ë¯¼ê°, ê·¹ë‹¨ê°’ì´ ìˆìœ¼ë©´ quantization error ì»¤ì§

---

### 2. PercentileObserver - ì¶”ì²œ ì„¤ì • â­

```yaml
# configs/quant_config_percentile.yaml
default:
  observer_type: PercentileObserver
  percentile_alpha: 0.9995      # í•µì‹¬ íŒŒë¼ë¯¸í„° 1
  percentile_sigma: 0.1         # í•µì‹¬ íŒŒë¼ë¯¸í„° 2

attn_layer:
  attn_qkv_output:
    observer_type: PercentileObserver
    percentile_alpha: 0.999     # ë” aggressive clipping
    percentile_sigma: 0.2       # ë” ë¹ ë¥¸ ì—…ë°ì´íŠ¸
```

**íŒŒë¼ë¯¸í„° ì„¤ëª…:**
- `percentile_alpha`: ì‚¬ìš©í•  ë°ì´í„° ë²”ìœ„ (0.999 = 99.9% ë²”ìœ„)
  - ë†’ì„ìˆ˜ë¡ outlier í¬í•¨ (ë³´ìˆ˜ì )
  - ë‚®ì„ìˆ˜ë¡ aggressive clipping (ì •í™•ë„ í–¥ìƒ)
- `percentile_sigma`: EMA ì—…ë°ì´íŠ¸ ì†ë„ (0 ~ 1)
  - ë†’ì„ìˆ˜ë¡ ìƒˆ ë°°ì¹˜ ì˜í–¥ í¬ê²Œ (ë¹ ë¥¸ ì ì‘)
  - ë‚®ì„ìˆ˜ë¡ ì´ì „ í†µê³„ ìœ ì§€ (ì•ˆì •ì )

**ì¥ì **: Outlier ê°•ê±´, ì†ë„/ì •í™•ë„ ê· í˜• ìš°ìˆ˜
**ë‹¨ì **: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”

---

### 3. OmseObserver - ì •ë°€ ì„¤ì •

```yaml
# configs/quant_config_omse.yaml
layers:
  attn_qkv:
    observer_type: OmseObserver
    # ë³„ë„ íŒŒë¼ë¯¸í„° ì—†ìŒ (grid search 100 iterations ê³ ì •)

  mlp_fc1:
    observer_type: OmseObserver

# âš ï¸ Activationì—ëŠ” ì‚¬ìš© ê°€ëŠ¥í•˜ë‚˜ ê³„ì‚°ëŸ‰ í¼
attn_layer:
  attn_qkv_output:
    observer_type: OmseObserver  # ê°€ëŠ¥í•˜ì§€ë§Œ ëŠë¦¼
```

**íŠ¹ì§•:**
- Grid searchë¡œ 100ê°œ threshold í›„ë³´ íƒìƒ‰
- L2 loss (MSE) ìµœì†Œí™”
- ê³„ì‚°ëŸ‰ ë§ì•„ calibration ì‹œê°„ 10ë°° ì´ìƒ ì¦ê°€
- Weight quantizationì— íŠ¹íˆ íš¨ê³¼ì 

**ì¥ì **: ê°€ì¥ ì •í™•í•œ weight quantization
**ë‹¨ì **: ë§¤ìš° ëŠë¦¼, ì‹¤ì‹œê°„ calibration ë¶€ì í•©

---

### 4. KLObserver - ë¶„í¬ ìœ ì§€ ì„¤ì •

```yaml
# configs/quant_config_kl.yaml
default:
  observer_type: KLObserver
  kl_bins: 2048                 # íˆìŠ¤í† ê·¸ë¨ bin ìˆ˜

attn_layer:
  intSoft:
    observer_type: KLObserver
    kl_bins: 4096               # softmaxëŠ” ë” ì„¸ë°€í•œ bin
    calibration_mode: layer_wise

  attn_qkv_output:
    observer_type: KLObserver
    kl_bins: 2048
    calibration_mode: channel_wise  # per-channel ìµœì í™”
```

**íŒŒë¼ë¯¸í„° ì„¤ëª…:**
- `kl_bins`: íˆìŠ¤í† ê·¸ë¨ bin ìˆ˜ (512 ~ 8192)
  - ë†’ì„ìˆ˜ë¡ ì •ë°€í•œ ë¶„í¬ ì¶”ì •
  - ë©”ëª¨ë¦¬ì™€ ê³„ì‚°ëŸ‰ ì¦ê°€

**íŠ¹ì§•:**
- KL divergence ìµœì†Œí™” = ë¶„í¬ ë³´ì¡´
- Softmax, LayerNorm ê°™ì€ ë¯¼ê°í•œ ì—°ì‚°ì— ì í•©
- Symmetric quantizationë§Œ ì§€ì›

**ì¥ì **: ë¶„í¬ íŠ¹ì„± ë³´ì¡´, ì´ë¡ ì ìœ¼ë¡œ ìš°ìˆ˜
**ë‹¨ì **: ê³„ì‚° ëŠë¦¼, asymmetric ë¯¸ì§€ì›

---

## ì½”ë“œ ì‚¬ìš© ì˜ˆì‹œ

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from models.vit_block import QuantTimmVitBlock

# 1. YAML ê¸°ë°˜ ì„¤ì • (ê¶Œì¥)
quant_block = QuantTimmVitBlock(
    block=original_vit_block,
    quant_config='configs/quant_config_percentile.yaml',
    enable_profiling=False
)

# 2. Calibration
for batch in calibration_loader:
    with torch.no_grad():
        quant_block.calibration(batch)

# 3. Quantization parameter ê³„ì‚°
quant_block.compute_quant_params()

# 4. Inference
quant_block.mode = 'quantized'
output = quant_block(input_tensor)
```

### Observer íƒ€ì…ë³„ ì„¸íŒ…

```python
# MinmaxObserver - ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘
observer = MinmaxObserver(
    bit_type=BitType(bits=8, signed=True, name='int8'),
    module_type='activation',
    calibration_mode='channel_wise'
)

# PercentileObserver - í”„ë¡œë•ì…˜ ê¶Œì¥
observer = PercentileObserver(
    bit_type=BitType(bits=8, signed=True, name='int8'),
    module_type='activation',
    calibration_mode='channel_wise',
    percentile_alpha=0.9995,  # 99.95% ë°ì´í„° ì‚¬ìš©
    percentile_sigma=0.1      # EMA ì—…ë°ì´íŠ¸ ì†ë„
)

# OmseObserver - ìµœëŒ€ ì •í™•ë„
observer = OmseObserver(
    bit_type=BitType(bits=8, signed=True, name='int8'),
    module_type='conv_weight',
    calibration_mode='channel_wise'
)

# KLObserver - ë¶„í¬ ë³´ì¡´
observer = KLObserver(
    bit_type=BitType(bits=8, signed=True, name='int8'),
    module_type='activation',
    calibration_mode='layer_wise',
    kl_bins=2048
)
```

### Manual Calibration

```python
# Observer ì§ì ‘ ì‚¬ìš© (low-level API)
observer = PercentileObserver(
    bit_type=BitType(bits=8, signed=True, name='int8'),
    module_type='activation',
    calibration_mode='channel_wise',
    percentile_alpha=0.999,
    percentile_sigma=0.1
)

# ë°°ì¹˜ë§ˆë‹¤ í†µê³„ ì—…ë°ì´íŠ¸
for batch in calibration_data:
    activations = model.get_activations(batch)  # [N, C, H, W]
    observer.update(activations)

# Quantization parameters ê³„ì‚°
scale, zero_point = observer.get_quantization_params()

# ìˆ˜ë™ quantization
quantized = torch.clamp(
    torch.round(activations / scale) + zero_point,
    min=bit_type.lower_bound,
    max=bit_type.upper_bound
)
dequantized = (quantized - zero_point) * scale
```

---

## íŒŒë¼ë¯¸í„° ìƒì„¸ ì„¤ëª…

### ê³µí†µ íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | íƒ€ì… | ì„¤ëª… | ê¸°ë³¸ê°’ |
|---------|------|------|--------|
| `bit_type` | BitType | ì–‘ìí™” bit ì„¤ì • (bits, signed, name) | - |
| `module_type` | str | ë ˆì´ì–´ íƒ€ì… (`activation`, `conv_weight`, `linear_weight`) | - |
| `calibration_mode` | str | `layer_wise` (ì „ì²´), `channel_wise` (ì±„ë„ë³„) | `channel_wise` |

### PercentileObserver íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | íƒ€ì… | ë²”ìœ„ | ì„¤ëª… | ê¶Œì¥ê°’ |
|---------|------|------|------|--------|
| `percentile_alpha` | float | 0.9 ~ 0.99999 | ì‚¬ìš©í•  ë°ì´í„° ë¹„ìœ¨ | `0.9995` (activation)<br>`0.999` (weight) |
| `percentile_sigma` | float | 0.0 ~ 1.0 | EMA ì—…ë°ì´íŠ¸ ì†ë„ | `0.1` |

**íŠœë‹ ê°€ì´ë“œ:**

```python
# Conservative (outlier í¬í•¨, ì•ˆì •ì )
percentile_alpha=0.99999, percentile_sigma=0.01

# Balanced (ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ìµœì )
percentile_alpha=0.9995, percentile_sigma=0.1

# Aggressive (ì •í™•ë„ ìš°ì„ , ë¹ ë¥¸ ì ì‘)
percentile_alpha=0.99, percentile_sigma=0.3
```

### KLObserver íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | íƒ€ì… | ë²”ìœ„ | ì„¤ëª… | ê¶Œì¥ê°’ |
|---------|------|------|------|--------|
| `kl_bins` | int | 512 ~ 8192 | íˆìŠ¤í† ê·¸ë¨ bin ìˆ˜ | `2048` |

**bin ìˆ˜ ì„ íƒ:**
- `512`: ë¹ ë¥´ì§€ë§Œ ì •ë°€ë„ ë‚®ìŒ
- `2048`: ê· í˜• (ê¸°ë³¸ê°’)
- `4096`: Softmax ë“± ë¯¼ê°í•œ ì—°ì‚°
- `8192`: ìµœëŒ€ ì •ë°€ë„ (ë©”ëª¨ë¦¬/ì†ë„ í¬ìƒ)

---

## ì‹¤ì „ ê°€ì´ë“œ

### 1. Observer ì„ íƒ í”Œë¡œìš°ì°¨íŠ¸

```
í”„ë¡œí† íƒ€ì´í•‘ ë‹¨ê³„?
â”œâ”€ YES â†’ MinmaxObserver (ë¹ ë¥¸ ê²€ì¦)
â””â”€ NO
   â””â”€ Weight ë˜ëŠ” Activation?
      â”œâ”€ Weight
      â”‚  â”œâ”€ ì •í™•ë„ ìµœìš°ì„  â†’ OmseObserver
      â”‚  â””â”€ ì†ë„/ì •í™•ë„ ê· í˜• â†’ PercentileObserver (alpha=0.999)
      â””â”€ Activation
         â”œâ”€ Softmax/LayerNorm â†’ KLObserver (layer_wise)
         â”œâ”€ ì¼ë°˜ activation â†’ PercentileObserver (alpha=0.9995)
         â””â”€ ReLU ê³„ì—´ â†’ PercentileObserver (alpha=0.99, aggressive)
```

### 2. Layerë³„ ì¶”ì²œ ì„¤ì •

```yaml
# configs/recommended_config.yaml

# ==================== Attention Block ====================
layers:
  # QKV projection - weightëŠ” OMSEë¡œ ì •ë°€í•˜ê²Œ
  attn_qkv:
    observer_type: OmseObserver
    calibration_mode: channel_wise

  # Attention projection - weightëŠ” OMSE
  attn_proj:
    observer_type: OmseObserver
    calibration_mode: channel_wise

attn_layer:
  # QKV output - activationì€ percentile
  attn_qkv_output:
    observer_type: PercentileObserver
    percentile_alpha: 0.999
    percentile_sigma: 0.1

  # Softmax - ë¶„í¬ ìœ ì§€ê°€ ì¤‘ìš”
  intSoft:
    observer_type: KLObserver
    kl_bins: 4096
    calibration_mode: layer_wise

  # Attention output - percentile
  sv_attn:
    observer_type: PercentileObserver
    percentile_alpha: 0.9995
    percentile_sigma: 0.1

# ==================== MLP Block ====================
layers:
  # MLP weights - OMSE
  mlp_fc1:
    observer_type: OmseObserver

  mlp_fc2:
    observer_type: OmseObserver

attn_layer:
  # GELU activation - percentile
  mlp_act:
    observer_type: PercentileObserver
    percentile_alpha: 0.999
    percentile_sigma: 0.2
```

### 3. Calibration ë°ì´í„°ì…‹ í¬ê¸°

```python
# Observerë³„ ê¶Œì¥ calibration ë°°ì¹˜ ìˆ˜
calibration_batches = {
    'MinmaxObserver': 10,       # 10 ë°°ì¹˜ë©´ ì¶©ë¶„
    'PercentileObserver': 50,   # 50 ë°°ì¹˜ ê¶Œì¥
    'OmseObserver': 20,         # Grid search ë¹„ìš© ë•Œë¬¸ì— ì ê²Œ
    'KLObserver': 100           # ë¶„í¬ ì¶”ì • ìœ„í•´ ë§ì´ í•„ìš”
}
```

### 4. ë””ë²„ê¹… íŒ

```python
# Observer í†µê³„ í™•ì¸
print(f"Max val: {observer.max_val}")
print(f"Min val: {observer.min_val}")

scale, zero_point = observer.get_quantization_params()
print(f"Scale: {scale}")
print(f"Zero point: {zero_point}")

# Quantization error ì¸¡ì •
original = activation
quantized = ((original / scale).round() + zero_point).clamp(
    bit_type.lower_bound, bit_type.upper_bound
)
dequantized = (quantized - zero_point) * scale

mse = ((original - dequantized) ** 2).mean()
print(f"MSE: {mse.item()}")
```

### 5. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ImageNet calibration 100 ë°°ì¹˜)

| Observer | Calibration ì‹œê°„ | Accuracy Drop | ë©”ëª¨ë¦¬ ì‚¬ìš© |
|----------|------------------|---------------|-------------|
| MinmaxObserver | 2.3s | -1.2% | 100 MB |
| PercentileObserver | 5.1s | -0.3% | 150 MB |
| OmseObserver | 48.7s | -0.1% | 500 MB |
| KLObserver | 32.4s | -0.2% | 300 MB |

### 6. ì¼ë°˜ì ì¸ ì‹¤ìˆ˜

```yaml
# âŒ ì˜ëª»ëœ ì„¤ì •
attn_layer:
  intSoft:
    observer_type: MinmaxObserver  # Softmaxì— MinMaxëŠ” ë¶€ì í•©
    calibration_mode: channel_wise # SoftmaxëŠ” layer_wise ê¶Œì¥

# âœ… ì˜¬ë°”ë¥¸ ì„¤ì •
attn_layer:
  intSoft:
    observer_type: KLObserver
    calibration_mode: layer_wise
    kl_bins: 4096
```

```python
# âŒ Calibration ì „ì— compute_quant_params() í˜¸ì¶œ
quant_block.compute_quant_params()  # í†µê³„ ì—†ìŒ!
quant_block.calibration(data)

# âœ… ì˜¬ë°”ë¥¸ ìˆœì„œ
quant_block.calibration(data)        # 1. í†µê³„ ìˆ˜ì§‘
quant_block.compute_quant_params()   # 2. íŒŒë¼ë¯¸í„° ê³„ì‚°
```

---

## ì°¸ê³  ìë£Œ

- **Observer êµ¬í˜„**: [models/ptq/layer_observer/](../models/ptq/layer_observer/)
- **YAML ì„¤ì • ì˜ˆì‹œ**: [configs/activation_config.yaml](../configs/activation_config.yaml)
- **ViT Block ì‚¬ìš© ì˜ˆì‹œ**: [test_vit_block.py](../test_vit_block.py)
- **CLAUDE.md**: [í”„ë¡œì íŠ¸ ê°œìš”](../CLAUDE.md)

---

**ì‘ì„±ì¼**: 2026-01-25
**ë²„ì „**: v1.0